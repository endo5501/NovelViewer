## Context

現在の `LlmSummaryService` は `TextSearchService.searchWithContext()` で全マッチを収集した後、`LlmPromptBuilder` で先頭10件のコンテキストのみをプロンプトに含めてLLMに送信している。頻出語の場合、物語序盤のコンテキストだけで10件に達し、中盤〜終盤の重要情報が欠落する。

対応するLLMバックエンドはOllamaとOpenAI互換APIの2種類。コンテキストウィンドウサイズはモデルによって異なり、アプリ側では把握していない。

## Goals / Non-Goals

**Goals:**
- 全マッチのコンテキストをLLMに渡せるようにする
- モデルのコンテキストウィンドウに依存しない設計にする
- 既存のネタバレあり/なし制御を維持する

**Non-Goals:**
- LLMクライアントインターフェース（`LlmClient`）の変更
- UIの変更（進捗表示の追加など）
- トークナイザーによる厳密なトークン数計算

## Decisions

### 1. Map-Reduce方式のパイプライン導入

**決定**: コンテキストをチャンク分割し、各チャンクから事実を箇条書き抽出、最終的に要約を生成する多段階パイプラインを導入する。

**理由**: 単純に上限を増やすだけではモデルのコンテキストウィンドウを超える可能性がある。Map-Reduce方式なら、コンテキストウィンドウサイズに関係なく任意の量の情報を処理できる。

**代替案**:
- 上限を増やすだけ（30件等） → モデルによってはウィンドウを超える。根本解決にならない
- サンプリング（均等抽出） → 情報の欠落が発生する

### 2. 文字数ベースのチャンク分割（約4000文字）

**決定**: コンテキストを約4000文字ごとにチャンク分割する。コンテキスト単位（1つのマッチ）の途中では分割せず、コンテキスト境界で分割する。

**理由**: 日本語テキストでは1文字≒1〜2トークンの近似が実用上妥当。4000文字なら多くのモデルでコンテキストウィンドウに収まる。トークナイザーに依存しないため実装がシンプル。

**代替案**:
- トークン数ベース → モデルごとにトークナイザーが異なり、実装が複雑化する
- 固定件数（例: 50件/チャンク） → コンテキストの長さにばらつきがあるため不適切

### 3. 再帰的な事実集約

**決定**: Stage 1で抽出した箇条書き群の合計が4000文字を超える場合、再度チャンク分割→事実抽出を繰り返す。4000文字以内に収まるまで再帰する。無限ループ防止のため再帰上限を5回とする。

**理由**: 大量のコンテキストでも確実に収束する。各ステージで情報が圧縮されるため、再帰は有限回で終了する。

### 4. 入力データでのネタバレ制御

**決定**: ネタバレあり/なしの分岐は、パイプラインに渡すコンテキストのフィルタリング段階で制御する。パイプライン内部のプロンプトやロジックは共通とする。

**理由**: LLMへの指示だけでネタバレを防ぐのは不確実。入力データに未来のエピソード情報が含まれていなければ、原理的にネタバレは発生しない。

### 5. プロンプト構成

**決定**: 2種類のプロンプトを用意する。
- **事実抽出プロンプト**: チャンクから用語に関する事実を箇条書きで列挙させる
- **最終要約プロンプト**: 事実群から用語の説明を1〜2文で生成させる

両プロンプトともネタバレあり/なしの区別はしない。

## Risks / Trade-offs

- **処理時間の増加** → チャンク数に比例してLLM呼び出し回数が増える。頻出語では数十回の呼び出しになりうる。UIでの進捗表示は本changeのスコープ外とするが、将来的に検討が必要
- **APIコストの増加** → OpenAI互換APIを使用している場合、呼び出し回数増加がコストに直結する。ユーザーが認識できるようにすべきだが、UI変更はスコープ外
- **再帰が上限に達した場合** → 5回の再帰でも収束しなければ、その時点の事実群で最終要約を生成する。情報の完全性は保証されないが、実用上は十分
- **LLMの事実抽出精度** → 小型モデルでは箇条書き抽出の品質が低い可能性がある。これはモデル選択の問題でありアプリ側では対処しない
