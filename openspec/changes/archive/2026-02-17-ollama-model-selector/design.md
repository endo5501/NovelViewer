## Context

現在、Ollamaのモデル名は設定ダイアログの`TextField`で手入力する形式。`OllamaClient`は`baseUrl`と`model`を受け取り、`/api/generate`エンドポイントに通信する。Ollama APIの`GET /api/tags`エンドポイントでインストール済みモデル一覧を取得できる。

## Goals / Non-Goals

**Goals:**
- Ollamaサーバからモデル一覧を取得し、ドロップダウンで選択可能にする
- Ollama選択時に自動取得、更新ボタンで再取得可能にする
- サーバ未起動・接続失敗時は適切なエラーを表示する

**Non-Goals:**
- 手入力へのフォールバック対応
- モデルのパラメータサイズ等の詳細情報の表示
- OpenAI互換APIのモデル一覧取得対応

## Decisions

### 1. モデル一覧取得ロジックの配置

**決定**: `OllamaClient`にstaticメソッド`fetchModels`を追加する。

**理由**: `OllamaClient`は既にOllama APIとの通信を担っており、同じ`baseUrl`を使う。モデル一覧取得はクライアントインスタンス生成前に必要なため、staticメソッドとする。`http.Client`はパラメータで受け取り、テスタビリティを確保する。

**代替案**: 別クラス`OllamaModelService`を作る案もあるが、単一メソッドのために新クラスを作るのは過剰。

### 2. UI設計

**決定**: Ollamaプロバイダ選択時、モデル名の`TextField`を`DropdownButton`に置き換える。横に更新ボタン（`IconButton`）を配置する。

**理由**: 既存のプロバイダ選択UIと同じパターン（`DropdownButton`）を踏襲し、一貫性を保つ。OpenAI互換APIのモデル名入力は従来通り`TextField`のまま。

### 3. 状態管理

**決定**: 設定ダイアログ内のローカルState（`setState`）でモデル一覧とローディング/エラー状態を管理する。

**理由**: モデル一覧は設定ダイアログでのみ使用する一時的なデータであり、Riverpodプロバイダにする必要はない。既存のダイアログ実装も`_llmProvider`等をローカルStateで管理している。

### 4. エラーハンドリング

**決定**: 接続失敗時はドロップダウンの代わりにエラーメッセージを表示する。

**理由**: サーバ未起動時はLLM機能自体が使えないため、手入力フォールバックは不要。

## Risks / Trade-offs

- **[リスク] Ollamaサーバのレスポンスが遅い場合、UI がブロックされる** → ローディングインジケータを表示して対応
- **[リスク] モデル一覧取得後にモデルが追加/削除される** → 更新ボタンで再取得可能にして対応
